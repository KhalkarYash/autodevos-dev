from __future__ import annotations

import os
from typing import Optional

from .utils import log


class BaseLLM:
    def generate_code(self, prompt: str, system: Optional[str] = None, temperature: float = 0.2, max_tokens: int = 4096) -> str:
        raise NotImplementedError


class MockLLM(BaseLLM):
    def generate_code(self, prompt: str, system: Optional[str] = None, temperature: float = 0.0, max_tokens: int = 4096) -> str:
        # Very simple template-based responses to keep the system functional offline.
        p = prompt.lower()
        if "frontend" in p:
            return "// Frontend component placeholder generated by MockLLM\nexport const Hello = () => <div>Hello from AutoDevOS Mock Frontend</div>;\n"
        if "backend" in p:
            return "// Backend route placeholder generated by MockLLM\nexport const route = (req, res) => res.json({ok: true});\n"
        if "test" in p:
            return "// Jest test placeholder generated by MockLLM\ntest('smoke', () => expect(true).toBe(true));\n"
        if "doc" in p:
            return "# Documentation\nThis documentation was generated by MockLLM.\n"
        return "// Generic code placeholder from MockLLM\n"


class GeminiLLM(BaseLLM):
    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-1.5-flash") -> None:
        self.api_key = api_key or os.environ.get("GEMINI_API_KEY")
        self.model = model
        self._client = None
        if self.api_key:
            try:
                import google.generativeai as genai  # type: ignore
                genai.configure(api_key=self.api_key)
                self._client = genai.GenerativeModel(self.model)
                log.info("Gemini LLM initialized.")
            except Exception as e:
                log.warning(f"Failed to initialize Gemini LLM, falling back to MockLLM: {e}")
                self._client = None
        else:
            log.warning("GEMINI_API_KEY not set. Falling back to MockLLM.")

    def generate_code(self, prompt: str, system: Optional[str] = None, temperature: float = 0.2, max_tokens: int = 8192) -> str:
        if self._client is None:
            return MockLLM().generate_code(prompt, system, temperature, max_tokens)
        try:
            # Some SDKs accept a list of parts; keep it simple for now
            content = prompt if system is None else f"System: {system}\nUser: {prompt}"
            resp = self._client.generate_content(content, generation_config={"temperature": temperature, "max_output_tokens": max_tokens})
            text = resp.text or ""
            return text
        except Exception as e:
            log.error(f"Gemini error, using MockLLM fallback: {e}")
            return MockLLM().generate_code(prompt, system, temperature, max_tokens)


def make_llm(prefer_gemini: bool = True) -> BaseLLM:
    if prefer_gemini:
        return GeminiLLM()
    return MockLLM()
