from __future__ import annotations

import os
import time
from typing import Optional, Iterator
from functools import wraps
from pathlib import Path

from .utils import log


class LLMConfigError(Exception):
    """Raised when LLM configuration is invalid."""
    pass


class LLMAPIError(Exception):
    """Raised when LLM API call fails."""
    pass


class BaseLLM:
    """Base class for LLM providers with retry and error handling support."""
    
    def generate_code(self, prompt: str, system: Optional[str] = None, temperature: float = 0.2, max_tokens: int = 4096) -> str:
        """Generate code from a prompt with optional system message."""
        raise NotImplementedError
    
    def generate_streaming(self, prompt: str, system: Optional[str] = None, temperature: float = 0.2, max_tokens: int = 4096) -> Iterator[str]:
        """Generate code with streaming response (optional for providers that support it)."""
        # Default implementation: return full response at once
        yield self.generate_code(prompt, system, temperature, max_tokens)


class MockLLM(BaseLLM):
    def generate_code(self, prompt: str, system: Optional[str] = None, temperature: float = 0.0, max_tokens: int = 4096) -> str:
        # Very simple template-based responses to keep the system functional offline.
        p = prompt.lower()
        if "frontend" in p:
            return "// Frontend component placeholder generated by MockLLM\nexport const Hello = () => <div>Hello from AutoDevOS Mock Frontend</div>;\n"
        if "backend" in p:
            return "// Backend route placeholder generated by MockLLM\nexport const route = (req, res) => res.json({ok: true});\n"
        if "test" in p:
            return "// Jest test placeholder generated by MockLLM\ntest('smoke', () => expect(true).toBe(true));\n"
        if "doc" in p:
            return "# Documentation\nThis documentation was generated by MockLLM.\n"
        return "// Generic code placeholder from MockLLM\n"


class GeminiLLM(BaseLLM):
    """Gemini LLM with production-grade retry logic, timeout, streaming, and secure key handling."""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "gemini-1.5-flash", 
                 max_retries: int = 3, timeout: float = 120.0, 
                 base_backoff: float = 2.0, max_backoff: float = 60.0) -> None:
        """
        Initialize Gemini LLM client.
        
        Args:
            api_key: API key (defaults to GEMINI_API_KEY env var)
            model: Model name
            max_retries: Maximum retry attempts
            timeout: Request timeout in seconds
            base_backoff: Base backoff time for exponential backoff
            max_backoff: Maximum backoff time
        """
        self.model = model
        self.max_retries = max_retries
        self.timeout = timeout
        self.base_backoff = base_backoff
        self.max_backoff = max_backoff
        self._client = None
        self._genai = None
        
        # Secure key loading with validation
        self.api_key = self._load_api_key(api_key)
        
        if self.api_key:
            self._initialize_client()
        else:
            log.warning("GEMINI_API_KEY not set. GeminiLLM will fall back to MockLLM.")
    
    def _load_api_key(self, api_key: Optional[str]) -> Optional[str]:
        """Load and validate API key from parameter or environment."""
        key = api_key or os.environ.get("GEMINI_API_KEY")
        
        if key and not key.strip():
            log.warning("Empty GEMINI_API_KEY detected, ignoring")
            return None
        
        if key:
            # Basic validation: check key format (starts with expected prefix)
            if not key.startswith("AIza"):
                log.warning("GEMINI_API_KEY has unexpected format (should start with 'AIza')")
        
        return key
    
    def _initialize_client(self) -> None:
        """Initialize Gemini client with error handling."""
        try:
            import google.generativeai as genai  # type: ignore
            self._genai = genai
            genai.configure(api_key=self.api_key)
            
            # Initialize model with safety and generation config
            self._client = genai.GenerativeModel(
                self.model,
                safety_settings={
                    "HARASSMENT": "BLOCK_NONE",
                    "HATE_SPEECH": "BLOCK_NONE", 
                    "SEXUALLY_EXPLICIT": "BLOCK_NONE",
                    "DANGEROUS_CONTENT": "BLOCK_NONE"
                }
            )
            log.info(f"Gemini LLM initialized: model={self.model}, max_retries={self.max_retries}, timeout={self.timeout}s")
        except ImportError as e:
            log.error(f"google-generativeai package not installed: {e}")
            self._client = None
            raise LLMConfigError("google-generativeai package required. Install with: pip install google-generativeai") from e
        except Exception as e:
            log.error(f"Failed to initialize Gemini client: {e}")
            self._client = None
            raise LLMConfigError(f"Gemini initialization failed: {e}") from e

    def _calculate_backoff(self, attempt: int) -> float:
        """Calculate exponential backoff with jitter."""
        backoff = min(self.base_backoff * (2 ** attempt), self.max_backoff)
        jitter = (time.time() % 1) * 0.1 * backoff  # 0-10% jitter
        return backoff + jitter

    def _retry_with_backoff(self, func, *args, **kwargs):
        """Execute function with exponential backoff retry logic."""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                last_error = e
                error_msg = str(e).lower()
                
                # Check if error is retryable
                retryable = any(keyword in error_msg for keyword in [
                    'timeout', 'rate limit', 'quota', 'service unavailable', 
                    '429', '500', '502', '503', '504'
                ])
                
                if not retryable or attempt >= self.max_retries - 1:
                    log.error(f"Gemini API failed after {attempt + 1} attempt(s): {e}")
                    raise LLMAPIError(f"Gemini API error: {e}") from e
                
                wait_time = self._calculate_backoff(attempt)
                log.warning(f"Gemini API error (attempt {attempt + 1}/{self.max_retries}): {e}. "
                           f"Retrying in {wait_time:.2f}s...")
                time.sleep(wait_time)
        
        raise LLMAPIError(f"Gemini API failed after {self.max_retries} retries") from last_error
    
    def generate_code(self, prompt: str, system: Optional[str] = None, temperature: float = 0.2, max_tokens: int = 8192) -> str:
        """Generate code with production error handling and fallback."""
        if self._client is None:
            log.debug("Gemini client not initialized, using MockLLM fallback")
            return MockLLM().generate_code(prompt, system, temperature, max_tokens)
        
        try:
            def _generate():
                # Construct full prompt
                if system:
                    full_prompt = f"{system}\n\n{prompt}"
                else:
                    full_prompt = prompt
                
                generation_config = {
                    "temperature": temperature,
                    "max_output_tokens": max_tokens,
                    "top_p": 0.95,
                    "top_k": 40,
                }
                
                # Generate with timeout handling
                start_time = time.time()
                response = self._client.generate_content(
                    full_prompt, 
                    generation_config=generation_config,
                    request_options={"timeout": self.timeout}
                )
                
                elapsed = time.time() - start_time
                
                # Extract text from response
                if hasattr(response, 'text'):
                    text = response.text
                elif hasattr(response, 'candidates') and response.candidates:
                    text = response.candidates[0].content.parts[0].text
                else:
                    text = str(response)
                
                log.debug(f"Gemini generation completed in {elapsed:.2f}s ({len(text)} chars)")
                return text or ""
            
            return self._retry_with_backoff(_generate)
            
        except LLMAPIError:
            raise  # Re-raise API errors
        except Exception as e:
            log.error(f"Unexpected Gemini error: {e}, using MockLLM fallback")
            return MockLLM().generate_code(prompt, system, temperature, max_tokens)
    
    def generate_streaming(self, prompt: str, system: Optional[str] = None, temperature: float = 0.2, max_tokens: int = 8192) -> Iterator[str]:
        """Generate code with streaming response and error handling."""
        if self._client is None:
            yield MockLLM().generate_code(prompt, system, temperature, max_tokens)
            return
        
        try:
            # Construct full prompt
            if system:
                full_prompt = f"{system}\n\n{prompt}"
            else:
                full_prompt = prompt
            
            generation_config = {
                "temperature": temperature,
                "max_output_tokens": max_tokens,
                "top_p": 0.95,
                "top_k": 40,
            }
            
            response = self._client.generate_content(
                full_prompt, 
                generation_config=generation_config, 
                stream=True,
                request_options={"timeout": self.timeout}
            )
            
            for chunk in response:
                if hasattr(chunk, 'text') and chunk.text:
                    yield chunk.text
                    
        except Exception as e:
            log.error(f"Gemini streaming failed: {e}, falling back to non-streaming")
            yield MockLLM().generate_code(prompt, system, temperature, max_tokens)


def make_llm(prefer_gemini: bool = True) -> BaseLLM:
    if prefer_gemini:
        return GeminiLLM()
    return MockLLM()
